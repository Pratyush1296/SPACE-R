{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010221ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import networkx as nx\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.matutils import cossim, jaccard\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e329251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32059299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8578ac",
   "metadata": {},
   "source": [
    "### Getting data and Stopword Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#                                                '''Edgelist import'''\n",
    "citation_edges = pd.read_csv('C:/Users/Pratyush Yadav/Desktop/IITM/Research/IPM Exp/AMiner_IS/JOI_Revise/cite_edges.csv')\n",
    "# citation_edges = pd.read_csv('/Users/pratyushyadav/Desktop/Citation Final/AMiner/Physics/Nodes1/Data/cite_edges.csv')\n",
    "# citation_edges = pd.read_csv('/Users/pratyushyadav/Desktop/Citation Final/AMiner/Mops/Nodes1/Data/cite_edges.csv')\n",
    "\n",
    "                        \n",
    "#                    '''filtered attributes with n_ref and n_cite, used for PCA''' \n",
    "pca_factors2 = pd.read_pickle('C:/Users/Pratyush Yadav/Desktop/IITM/Research/IPM Exp/AMiner_IS/new_pca_results.pkl')\n",
    "# cluster = pd.read_pickle('/Users/pratyushyadav/Desktop/Citation Final/AMiner/Physics/Nodes1/Data/Filtered_Data.pkl')\n",
    "# cluster = pd.read_pickle('/Users/pratyushyadav/Desktop/Citation Final/AMiner/Mops/Nodes1/Data/Filtered_Data.pkl')\n",
    "\n",
    "\n",
    "#                                '''input data (original subsetted data)'''\n",
    "citation_data = pd.read_csv('C:/Users/Pratyush Yadav/Downloads/Papers.csv')\n",
    "# citation_data = pd.read_pickle('/Users/pratyushyadav/Desktop/Citation Final/AMiner/Physics/Nodes1/Data/AMiner_physics.pkl')\n",
    "# citation_data = pd.read_pickle('/Users/pratyushyadav/Desktop/Citation Final/AMiner/Mops/Nodes1/Data/AMiner_mops.pkl')\n",
    "\n",
    "\n",
    "#                                    '''Creation of global graph'''\n",
    "# G1=nx.from_pandas_edgelist(citation_edges,'Source','Target','weight',create_using=nx.DiGraph())\n",
    "# G2=nx.from_pandas_edgelist(citation_edges,'Source','Target',create_using=nx.DiGraph())\n",
    "##Creating a list of stop words and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\",\"study\",\"algorithm\",\"paper\",\"method\",\"view\",\"approach\",\"research\",\"data\",\"study\",\"interest\",\"propose\"]\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "citation_data['references']=citation_data['references'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G1=nx.from_pandas_edgelist(citation_edges,'Source','Target','weight_best',create_using=nx.DiGraph())\n",
    "G2 = nx.from_pandas_edgelist(citation_edges,'Source','Target','weight_best',create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_5 = cluster[(cluster['n_ref']>=3)]['Paper_Id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32bd913",
   "metadata": {},
   "source": [
    "### Community Detection and Target Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae93db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "partition = community_louvain.best_partition(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=[]\n",
    "values=[]\n",
    "for i in partition.items():\n",
    "    key.append(i[0])\n",
    "    values.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "community_louvain.modularity(partition,G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c33627",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modularity['modularity class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modularity = pd.DataFrame({\n",
    "    'Id':key,\n",
    "    'modularity class':values\n",
    "})\n",
    "Modularity.to_csv('C:/Users/Pratyush Yadav/Desktop/IITM/Research/IPM Exp/AMiner_IS/JOI_Revise/Modularity_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f870ad5",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bba855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    lst4 = len(lst3)\n",
    "    return lst4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385e632",
   "metadata": {},
   "source": [
    "### Creating Ego and Popularity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ego_new(node,G,r):\n",
    "#     if (G.order()>30):\n",
    "    G_ego = nx.ego_graph(G,node,undirected=True,radius=math.ceil(r))\n",
    "    if (G_ego.order()<30):\n",
    "        G_ego=nx.ego_graph(G,node,undirected=True,radius=r,distance='weight_best')\n",
    "# #                     if (G_ego.order()<30):\n",
    "# #                         G_ego = nx.ego_graph(G,node,undirected=True,radius=math.ceil(2*r),distance='weight')\n",
    "#     else:\n",
    "#         G_ego=G\n",
    "    if (G_ego.order()<5):\n",
    "        G_ego=G\n",
    "    return G_ego\n",
    "\n",
    "def popularity_graph_test(G,citation_data,Node,test):\n",
    "    edge_list1 =[]\n",
    "    for line in nx.generate_edgelist(G, data=['weight_best']):\n",
    "        edge_list1.append(line)\n",
    "    source=[]\n",
    "    target=[]\n",
    "    weight=[]\n",
    "    for item in edge_list1:\n",
    "        if (item.split(\" \")[0]==Node):\n",
    "            if item.split(\" \")[1] not in test:\n",
    "                source.append(item.split(\" \")[0])\n",
    "                target.append(item.split(\" \")[1])\n",
    "                weight.append(float(item.split(\" \")[2]))\n",
    "        else:\n",
    "            source.append(item.split(\" \")[0])\n",
    "            target.append(item.split(\" \")[1])\n",
    "            weight.append(float(item.split(\" \")[2]))\n",
    "    cite_edges2 = pd.DataFrame({\n",
    "        'Source':source,\n",
    "        'Target':target,\n",
    "        'weight':weight\n",
    "    })\n",
    "    cite_edges2 =cite_edges2.sort_values(['weight'],ascending=True)\n",
    "    #cite_edges2['weight']= (cite_edges2['weight']-cite_edges2['weight'].min())/(cite_edges2['weight'].max()-cite_edges2['weight'].min())\n",
    "    '''Source1=[]\n",
    "    Target1=[]\n",
    "    Weight1=[]\n",
    "    for i in range(len(cite_edges2)):\n",
    "        if (cite_edges2.weight[i]<0):\n",
    "            Source1.append(cite_edges2.Target[i])\n",
    "            Target1.append(cite_edges2.Source[i])\n",
    "            Weight1.append(0-cite_edges2.weight[i])\n",
    "        else:\n",
    "            Source1.append(cite_edges2.Source[i])\n",
    "            Target1.append(cite_edges2.Target[i])\n",
    "            Weight1.append(cite_edges2.weight[i])\n",
    "    pop_edges= pd.DataFrame({\n",
    "        'Source':Source1,\n",
    "        'Target':Target1,\n",
    "        'Weight':Weight1\n",
    "    })'''\n",
    "    Pop_G=nx.from_pandas_edgelist(cite_edges2,'Source','Target','weight',create_using=nx.DiGraph())\n",
    "    return Pop_G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6bf3b",
   "metadata": {},
   "source": [
    "### Content Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_data['paper_id'] = citation_data['paper_id'].apply(lambda x: str(x))\n",
    "citation_data['Abstract'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_similarity(Pop_G,citation_data,Node):    \n",
    "    node3=[]\n",
    "    for node in Pop_G.nodes():\n",
    "        node3.append(str(node))\n",
    "    citation_data1=citation_data[citation_data['paper_id'].isin(node3)]\n",
    "    citation_data1.index = range(len(citation_data1))\n",
    "    #for content similarity calculation\n",
    "    LSI_data=[]\n",
    "    paper_id2=[]\n",
    "    for i in range(len(citation_data1)):\n",
    "        if(citation_data1.Abstract[i]!=''):\n",
    "            LSI_data.append(citation_data1.Abstract[i])\n",
    "        else:\n",
    "            LSI_data.append(citation_data1.Title[i])\n",
    "        paper_id2.append(citation_data1.paper_id[i])\n",
    "    Data_for_LSI = pd.DataFrame({\n",
    "        'Paper_ID':paper_id2,\n",
    "        'LSI_Data':LSI_data\n",
    "    })\n",
    "    processed_docs = Data_for_LSI['LSI_Data'].map(preprocess)\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        #print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "#     print(len(dictionary))\n",
    "    dictionary2 = copy.deepcopy(dictionary)\n",
    "    dictionary2.filter_extremes(no_below=15, no_above=0.5,keep_n=100000)\n",
    "#     print(len(dictionary2))\n",
    "    bow_corpus = [dictionary2.doc2bow(doc) for doc in processed_docs]\n",
    "    #bow_corpus[4310]\n",
    "    tfidf = gensim.models.TfidfModel(bow_corpus)  \n",
    "    #doc_bow = [(0, 1), (1, 1)]\n",
    "    #print(tfidf[doc_bow]) \n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "#     print(corpus_tfidf)\n",
    "    if (len(dictionary2)==0):\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "        tfidf = gensim.models.TfidfModel(bow_corpus)  \n",
    "        corpus_tfidf = tfidf[bow_corpus]\n",
    "        lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)  # initialize an LSI transformation\n",
    "        corpus_lsi = lsi[corpus_tfidf]\n",
    "        doc = citation_data[citation_data['paper_id']==str(Node)]['Title'].tolist()[0]\n",
    "        vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "        vec_lsi = lsi[vec_bow]\n",
    "        index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "        if (len(vec_lsi)==0):\n",
    "            doc = citation_data[citation_data['paper_id']==str(Node)]['Abstract'].tolist()[0]\n",
    "            vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "            vec_lsi = lsi[vec_bow]\n",
    "            index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "        #vec_lda = lda[vec_bow]\n",
    "        #print(vec_lsi)\n",
    "        if (len(vec_lsi)==0):\n",
    "            bow_corpus = [dictionary1.doc2bow(doc) for doc in processed_docs]\n",
    "            tfidf = gensim.models.TfidfModel(bow_corpus)  \n",
    "            corpus_tfidf = tfidf[bow_corpus]\n",
    "            doc = citation_data[citation_data['paper_id']==str(Node)]['Title'].tolist()[0]\n",
    "            vec_bow = dictionary1.doc2bow(doc.lower().split())\n",
    "            vec_lsi = lsi1[vec_bow]\n",
    "            index = gensim.similarities.MatrixSimilarity(lsi1[corpus_tfidf])\n",
    "    else:\n",
    "        lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary2, num_topics=10)  # initialize an LSI transformation\n",
    "        corpus_lsi = lsi[corpus_tfidf]\n",
    "        doc = citation_data[citation_data['paper_id']==str(Node)]['Title'].tolist()[0]\n",
    "        vec_bow = dictionary2.doc2bow(doc.lower().split())\n",
    "        vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "        index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "        if (len(vec_lsi)==0):\n",
    "            bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "            tfidf = gensim.models.TfidfModel(bow_corpus)  \n",
    "            corpus_tfidf = tfidf[bow_corpus]\n",
    "            lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)  # initialize an LSI transformation\n",
    "            corpus_lsi = lsi[corpus_tfidf]\n",
    "            doc = citation_data[citation_data['paper_id']==str(Node)]['Title'].tolist()[0]\n",
    "            vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "            vec_lsi = lsi[vec_bow]\n",
    "            index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "            if (len(vec_lsi)==0):\n",
    "                doc = citation_data[citation_data['paper_id']==str(Node)]['Abstract'].tolist()[0]\n",
    "                vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "                vec_lsi = lsi[vec_bow]\n",
    "                index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "            #vec_lda = lda[vec_bow]\n",
    "            #print(vec_lsi)\n",
    "            if (len(vec_lsi)==0):\n",
    "                bow_corpus = [dictionary1.doc2bow(doc) for doc in processed_docs]\n",
    "                tfidf = gensim.models.TfidfModel(bow_corpus)  \n",
    "                corpus_tfidf = tfidf[bow_corpus]\n",
    "                doc = citation_data[citation_data['paper_id']==str(Node)]['Title'].tolist()[0]\n",
    "                vec_bow = dictionary1.doc2bow(doc.lower().split())\n",
    "                vec_lsi = lsi1[vec_bow]\n",
    "                index = gensim.similarities.MatrixSimilarity(lsi1[corpus_tfidf])\n",
    "    sims = index[vec_lsi]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    pap_id1=[]\n",
    "    sim_score=[]\n",
    "    for i in range(len(sims)):\n",
    "        pap_id1.append(citation_data1[citation_data1.index==sims[i][0]]['paper_id'].tolist()[0])\n",
    "        sim_score.append(sims[i][1])\n",
    "    Content_sim = pd.DataFrame({\n",
    "        'Paper_ID':pap_id1,\n",
    "        'Similarity':sim_score\n",
    "    })\n",
    "#     Content_sim= Content_sim[Content_sim['Paper_ID']!=str(Node)]\n",
    "#     Content_sim.index = range(len(Content_sim))\n",
    "    #Content_sim['Similarity'] = (Content_sim['Similarity']-Content_sim['Similarity'].min())/(Content_sim['Similarity'].max()-Content_sim['Similarity'].min())\n",
    "    return Content_sim\n",
    "def preprocess(doc):#for i in range(len(citation_data.abstract)):\n",
    "    result=[]\n",
    "    #Remove punctuations\n",
    "    text = re.sub('[^a-zA-Z]', ' ', doc)\n",
    "\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "\n",
    "    # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "    #data_lemmatized = lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    #return data_lemmatized\n",
    "\n",
    "\n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()\n",
    "    #Lemmatisation\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            lem = WordNetLemmatizer()\n",
    "            text = lem.lemmatize(word) \n",
    "            result.append(text)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b658be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "textstem =importr('textstem')\n",
    "base = importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec95516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "\n",
    "#     elif nltk_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "\n",
    "    if nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#     elif nltk_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "\n",
    "    else:          \n",
    "        return None\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    words = np.array([i.lower() for i in sentence.split() if i not in stop_words])\n",
    "    lemmatized_sentence = textstem.lemmatize_words(words).tolist()   \n",
    "    nltk_tagged = nltk.pos_tag(lemmatized_sentence)\n",
    "    wordnet_tagged = wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     nltk_tagged = nltk.pos_tag(tokens)\n",
    "#     tokens = [i.lower() for i in nltk.word_tokenize(sentence)]\n",
    "#     nltk_tagged = nltk.pos_tag(tokens)\n",
    "#     wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     lemmatized_sentence = []\n",
    "\n",
    "#     for word, tag in wordnet_tagged:\n",
    "#         if word not in stop_words:\n",
    "#             if tag is None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 if word.endswith('ing'):\n",
    "#                     lemmatized_sentence.append(lemmatizer.lemmatize(word, wordnet.VERB))\n",
    "#                 else:\n",
    "#                     lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_final=[]\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is not None:\n",
    "            lemmatized_final.append(word)\n",
    "\n",
    "    return lemmatized_final\n",
    "\n",
    "def clean_text(abstract):\n",
    "    abstract = abstract.replace(\". \", \" \").replace(\", \", \" \").replace(\"! \", \" \")\\\n",
    "                       .replace(\"? \", \" \").replace(\": \", \" \").replace(\"; \", \" \")\\\n",
    "                       .replace(\"( \", \" \").replace(\") \", \" \").replace(\"| \", \" \").replace(\"/ \", \" \").replace(\"'\",\"\")\n",
    "    if \".\" in abstract or \",\" in abstract or \"!\" in abstract or \"?\" in abstract or \":\" in abstract or \";\" in abstract or \"(\" in abstract or \")\" in abstract or \"|\" in abstract or \"/\" in abstract:\n",
    "        abstract = abstract.replace(\".\", \" \").replace(\",\", \" \").replace(\"!\", \" \")\\\n",
    "                           .replace(\"?\", \" \").replace(\":\", \" \").replace(\";\", \" \")\\\n",
    "                           .replace(\"(\", \" \").replace(\")\", \" \").replace(\"|\", \" \").replace(\"/\", \" \").replace(\"'\",\"\")\n",
    "    abstract = abstract.replace(\"  \", \" \")\n",
    "    \n",
    "#     for word in list(set(stopwords.words(\"english\"))):\n",
    "#         abstract = abstract.replace(\" \" + word + \" \", \" \")\n",
    "\n",
    "\n",
    "    return lemmatize_sentence(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9395368",
   "metadata": {},
   "source": [
    "### Offline Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66f525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes = [int(i) for i in node_5]\n",
    "mod_classes=Modularity[Modularity['Id'].isin(nodes)]['modularity class'].unique().tolist()\n",
    "len(mod_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51322bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_node=[]\n",
    "radius=[]\n",
    "for i in range(len(mod_classes)):\n",
    "    node_list1 = Modularity[Modularity['modularity class']==mod_classes[i]]['Id'].tolist()\n",
    "    G_sub = G1.subgraph(node_list1)\n",
    "    sum1=0\n",
    "    n_path=0\n",
    "    for j in nx.shortest_path_length(G_sub):\n",
    "        for a,b in j[1].items():\n",
    "            sum1=sum1+b\n",
    "            if (b!=0):\n",
    "                n_path=n_path+1\n",
    "    r = sum1/n_path  \n",
    "    radius.append(r/2)\n",
    "    n_node.append(len(node_list1))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Community_info=pd.DataFrame({\n",
    "    'modularity class':mod_classes,\n",
    "    'Radius':radius,\n",
    "    'Nodes':n_node\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_class=[]\n",
    "node_info=[]\n",
    "n_node=[]\n",
    "radius=[]\n",
    "for i in range(len(node_5)):\n",
    "    Node = int(node_5[i])\n",
    "    class1=Modularity[Modularity['Id']==Node]['modularity class'].tolist()[0]\n",
    "    node_info.append(Node)\n",
    "    mod_class.append(class1)\n",
    "    radius.append(Community_info[Community_info['modularity class']==class1]['Radius'].tolist()[0])\n",
    "    n_node.append(Community_info[Community_info['modularity class']==class1]['Nodes'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modularity_1=pd.DataFrame({\n",
    "    'Node':node_info,\n",
    "    'Classes':mod_class,\n",
    "    'Nodes':n_node,\n",
    "    'Radius':radius\n",
    "})\n",
    "# Modularity_1#.to_pickle('/Users/pratyushyadav/Desktop/Citation Final/AMiner/IS&CV/Nodes1/Data/Modularity_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ed1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for evaluation purposes (general corpus)\n",
    "LSI_data1=[]\n",
    "paper_id3=[]\n",
    "for i in range(len(citation_data)):\n",
    "    if(citation_data.Abstract[i]!=''):\n",
    "        LSI_data1.append(citation_data.Abstract[i])\n",
    "    else:\n",
    "        LSI_data1.append(citation_data.Title[i])\n",
    "    paper_id3.append(citation_data.paper_id[i])\n",
    "#for evaluation\n",
    "Data_for_LSI1 = pd.DataFrame({\n",
    "    'Paper_ID':paper_id3,\n",
    "    'LSI_Data':LSI_data1\n",
    "})\n",
    "processed_docs1 = Data_for_LSI1['LSI_Data'].map(preprocess)\n",
    "dictionary1 = gensim.corpora.Dictionary(processed_docs1)\n",
    "count1 = 0\n",
    "for k, v in dictionary1.iteritems():\n",
    "    print(k, v)\n",
    "    count1 += 1\n",
    "    if count1 > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus1 = [dictionary1.doc2bow(doc) for doc in processed_docs1]\n",
    "#bow_corpus[4310]\n",
    "tfidf1 = gensim.models.TfidfModel(bow_corpus1)  \n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf1[doc_bow]) \n",
    "corpus_tfidf1 = tfidf1[bow_corpus1]\n",
    "lsi1 = gensim.models.LsiModel(corpus_tfidf1, id2word=dictionary1, num_topics=10)  # initialize an LSI transformation\n",
    "corpus_lsi1 = lsi1[corpus_tfidf1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b25eb",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc71695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa647b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster['Recency1'] = normalize(cluster['Recency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93593687",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_factors2['Quality1'] = normalize(pca_factors2['score_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399113a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_exec1=[]\n",
    "prec1=[]\n",
    "recall1=[]\n",
    "ave_prec3=[]\n",
    "rank3=[]\n",
    "ave_prec4=[]\n",
    "rank4=[]\n",
    "pap_ids=[]\n",
    "for i in range(2000):\n",
    "    Node = int(node_5[i]) #int values not str\n",
    "    print(Node, i)\n",
    "    #mod_class = Modularity[Modularity['Id']==Node]['modularity class'].tolist()[0]\n",
    "    #node_list1 = Modularity[Modularity['modularity class']==mod_class]['Id'].tolist()\n",
    "    list1 = citation_data[citation_data['paper_id']==str(Node)]['references'].tolist()[0]\n",
    "    mod_class = Modularity_1[Modularity_1['Node']==Node]['Classes'].tolist()[0]\n",
    "    if(type(mod_class)==int):\n",
    "        node_list1 = Modularity[Modularity['modularity class']==mod_class]['Id'].tolist()\n",
    "    else:\n",
    "        node_list1 = Modularity[Modularity['modularity class'].isin(mod_class)]['Id'].tolist()\n",
    "    r = Modularity_1[Modularity_1['Node']==Node]['Radius'].tolist()[0]\n",
    "    G_sub = G1.subgraph(node_list1)\n",
    "    prec=[]\n",
    "    recall=[]\n",
    "    time_exec=[]\n",
    "    ave_prec1=[]\n",
    "    ave_prec2=[]\n",
    "    rank1=[]\n",
    "    rank2=[]\n",
    "    for train_index, test_index in kf.split(list1):\n",
    "        test=[]\n",
    "        for i in test_index:\n",
    "            test.append(list1[i])\n",
    "#         tests.append(test)\n",
    "        start_time = time.time()\n",
    "        G_ego = create_ego_new(Node,G_sub,r)\n",
    "        Pop_G = popularity_graph_test(G_ego,citation_data,str(Node),test)\n",
    "        if (Pop_G.order()!=G_ego.order()):\n",
    "            for i in range(len(test)):\n",
    "                if int(test[i]) in list(G_ego.nodes()):\n",
    "                    Pop_G.add_node(test[i])\n",
    "        Content_sim = content_similarity(Pop_G,citation_data,Node)\n",
    "        recency=[]\n",
    "        popul=[]\n",
    "        for i in range(len(Content_sim)):\n",
    "            recency.append(cluster[cluster['Paper_Id']==int(Content_sim['Paper_ID'][i])]['Recency1'].tolist()[0])\n",
    "            popul.append(pca_factors2[pca_factors2['paper_id']==Content_sim['Paper_ID'][i]]['Quality1'].tolist()[0])\n",
    "        Content_sim['Recency1'] = recency\n",
    "        Content_sim['Quality1']= popul\n",
    "        Content_sim['Similarity1']=normalize(Content_sim['Similarity'])\n",
    "        Content_sim.fillna(0,inplace=True)\n",
    "#         Content_sim['Final_Score1'] = (Content_sim['Similarity1']+Content_sim['Recency1']+Content_sim['Quality1'])/3\n",
    "        Content_sim['Final_Score1'] = alpha*Content_sim['Similarity1']+beta*Content_sim['Recency1']+gamma*Content_sim['Quality1']\n",
    "        personalization = Content_sim.set_index('Paper_ID')['Final_Score1'].to_dict()\n",
    "#         if (Content_sim['Similarity'].unique().tolist()[0]!=0.0):\n",
    "#             personalization = Content_sim.set_index('Paper_ID')['Similarity'].to_dict()\n",
    "#         else:\n",
    "#             personalization=None\n",
    "        try:\n",
    "            calculated_page_rank = nx.pagerank(Pop_G, alpha = 0.35,weight='weight',personalization=personalization\n",
    "                                                  ,dangling=None)\n",
    "        except:\n",
    "            calculated_page_rank = nx.pagerank(Pop_G, alpha = 0.35,weight='weight',personalization=personalization,dangling=None)\n",
    "    # # most important sentences in descending order of importance\n",
    "        sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "        if (str(Node) in sentences):\n",
    "            sentences.remove(str(Node))\n",
    "        time_exec.append((time.time()-start_time))\n",
    "  #      print(time_exec)\n",
    "        paper_1=sentences[0:3]\n",
    "        paper_2=sentences[0:5]\n",
    "        paper_3=sentences[0:10]\n",
    "        paper_4=sentences[0:15]\n",
    "        paper_5=sentences[0:20]\n",
    "        paper_6=sentences[0:100]\n",
    "        pap_ids.append(paper_6)\n",
    "        len1 = intersection(paper_1,test)\n",
    "        len2 = intersection(paper_2,test)\n",
    "        len3 = intersection(paper_3,test)\n",
    "        len4 = intersection(paper_4,test)\n",
    "        len5 = intersection(paper_5,test)\n",
    "        exact_hit1=[(len1/len(test)),(len2/len(test)),(len3/len(test)),(len4/len(test)),(len5/len(test))]\n",
    "        exact_hit2=[len1/3,len2/5,len3/10,len4/15,len5/20]\n",
    "        len6 = intersection(paper_1,list1)\n",
    "        len7 = intersection(paper_2,list1)\n",
    "        len8 = intersection(paper_3,list1)\n",
    "        len9 = intersection(paper_4,list1)\n",
    "        len10 = intersection(paper_5,list1)\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        for i in range(len(paper_1)):\n",
    "            if paper_1[i] in test:\n",
    "                m=m+1\n",
    "                list_pr_1.append(m/(i+1))\n",
    "                if (n<2):\n",
    "                    rank_cv.append(1/(i+1))\n",
    "                    n=n+1\n",
    "        if (m!=0):\n",
    "            ave_prec1.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec1.append(0)\n",
    "        rank1.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        j=0\n",
    "        for i in range(len(paper_1)):\n",
    "            if paper_1[i] in list1:\n",
    "                if paper_1[i] in test:\n",
    "                    m=m+1\n",
    "                    j=j+1\n",
    "                    list_pr_1.append(m/j)\n",
    "                    if (n<2):\n",
    "                        rank_cv.append(1/j)\n",
    "                        n=n+1\n",
    "            else:\n",
    "                j=j+1\n",
    "        if (m!=0):\n",
    "            ave_prec2.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec2.append(0)\n",
    "        rank2.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        for i in range(len(paper_2)):\n",
    "            if paper_2[i] in test:\n",
    "                m=m+1\n",
    "                list_pr_1.append(m/(i+1))\n",
    "                if (n<2):\n",
    "                    rank_cv.append(1/(i+1))\n",
    "                    n=n+1\n",
    "        if (m!=0):\n",
    "            ave_prec1.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec1.append(0)\n",
    "        rank1.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        j=0\n",
    "        for i in range(len(paper_2)):\n",
    "            if paper_2[i] in list1:\n",
    "                if paper_2[i] in test:\n",
    "                    m=m+1\n",
    "                    j=j+1\n",
    "                    list_pr_1.append(m/j)\n",
    "                    if (n<2):\n",
    "                        rank_cv.append(1/j)\n",
    "                        n=n+1\n",
    "            else:\n",
    "                j=j+1\n",
    "        if (m!=0):\n",
    "            ave_prec2.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec2.append(0)\n",
    "        rank2.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        for i in range(len(paper_3)):\n",
    "            if paper_3[i] in test:\n",
    "                m=m+1\n",
    "                list_pr_1.append(m/(i+1))\n",
    "                if (n<2):\n",
    "                    rank_cv.append(1/(i+1))\n",
    "                    n=n+1\n",
    "        if (m!=0):\n",
    "            ave_prec1.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec1.append(0)\n",
    "        rank1.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        j=0\n",
    "        for i in range(len(paper_3)):\n",
    "            if paper_3[i] in list1:\n",
    "                if paper_3[i] in test:\n",
    "                    m=m+1\n",
    "                    j=j+1\n",
    "                    list_pr_1.append(m/j)\n",
    "                    if (n<2):\n",
    "                        rank_cv.append(1/j)\n",
    "                        n=n+1\n",
    "            else:\n",
    "                j=j+1\n",
    "        if (m!=0):\n",
    "            ave_prec2.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec2.append(0)\n",
    "        rank2.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        for i in range(len(paper_4)):\n",
    "            if paper_4[i] in test:\n",
    "                m=m+1\n",
    "                list_pr_1.append(m/(i+1))\n",
    "                if (n<2):\n",
    "                    rank_cv.append(1/(i+1))\n",
    "                    n=n+1\n",
    "        if (m!=0):\n",
    "            ave_prec1.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec1.append(0)\n",
    "        rank1.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        j=0\n",
    "        for i in range(len(paper_4)):\n",
    "            if paper_4[i] in list1:\n",
    "                if paper_4[i] in test:\n",
    "                    m=m+1\n",
    "                    j=j+1\n",
    "                    list_pr_1.append(m/j)\n",
    "                    if (n<2):\n",
    "                        rank_cv.append(1/j)\n",
    "                        n=n+1\n",
    "            else:\n",
    "                j=j+1\n",
    "        if (m!=0):\n",
    "            ave_prec2.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec2.append(0)\n",
    "        rank2.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        for i in range(len(paper_5)):\n",
    "            if paper_5[i] in test:\n",
    "                m=m+1\n",
    "                list_pr_1.append(m/(i+1))\n",
    "                if (n<2):\n",
    "                    rank_cv.append(1/(i+1))\n",
    "                    n=n+1\n",
    "        if (m!=0):\n",
    "            ave_prec1.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec1.append(0)\n",
    "        rank1.append(sum(rank_cv))\n",
    "        list_pr_1=[]\n",
    "        rank_cv=[]\n",
    "        n=1\n",
    "        m=0\n",
    "        j=0\n",
    "        for i in range(len(paper_5)):\n",
    "            if paper_5[i] in list1:\n",
    "                if paper_5[i] in test:\n",
    "                    m=m+1\n",
    "                    j=j+1\n",
    "                    list_pr_1.append(m/j)\n",
    "                    if (n<2):\n",
    "                        rank_cv.append(1/j)\n",
    "                        n=n+1\n",
    "            else:\n",
    "                j=j+1\n",
    "        if (m!=0):\n",
    "            ave_prec2.append(sum(list_pr_1)/m)\n",
    "        else:\n",
    "            ave_prec2.append(0)\n",
    "        rank2.append(sum(rank_cv))\n",
    "        prec.append(exact_hit2)\n",
    "        recall.append(exact_hit1)\n",
    "    #     dens.append(nx.density(Pop_G))\n",
    "    #     count=0\n",
    "    #     for i in dict(Pop_G.degree()).items():\n",
    "    #         count = count+i[1]\n",
    "    #     deg.append(count/(Pop_G.order()))\n",
    "    # #     ccoef.append(nx.clustering(Pop_G,Node))\n",
    "    # #     ccoef1.append(nx.average_clustering(Pop_G))\n",
    "    #     gccoef.append(nx.transitivity(Pop_G))\n",
    "    # #     print(nx.overall_reciprocity(G_ego))\n",
    "    # #     grc.append(nx.global_reaching_centrality(G_ego))\n",
    "    # #     dens1.append(nx.density(Pop_G))\n",
    "    #     sum1=0\n",
    "    #     n_path=0\n",
    "    #     for j in nx.shortest_path_length(Pop_G):\n",
    "    #         for a,b in j[1].items():\n",
    "    #             sum1=sum1+b\n",
    "    #             if (b!=0):\n",
    "    #                 n_path=n_path+1\n",
    "    #     apl.append(sum1/n_path)\n",
    "    prec1.append(prec)\n",
    "    recall1.append(recall)\n",
    "    ave_prec3.append(ave_prec1)\n",
    "    rank3.append(rank1)\n",
    "    ave_prec4.append(ave_prec2)\n",
    "    rank4.append(rank2)\n",
    "    time_exec1.append(time_exec)\n",
    "    # time_exec.append((time.time()-start_time))\n",
    "    # paper_1=sentences[0:10]\n",
    "    # paper_2=sentences[0:15]\n",
    "    # paper_3=sentences[0:20]\n",
    "    # paper_4=sentences[0:25]\n",
    "    # paper_5=sentences[0:30]\n",
    "    # paper_6=sentences[0:100]\n",
    "    # pap_ids.append(paper_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856495d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster['Popularity'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee3e2c",
   "metadata": {},
   "source": [
    "### Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9de5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_val=[]\n",
    "P_10=[]\n",
    "P_15=[]\n",
    "P_20=[]\n",
    "P_25=[]\n",
    "P_30=[]\n",
    "R_10=[]\n",
    "R_15=[]\n",
    "R_20=[]\n",
    "R_25=[]\n",
    "R_30=[]\n",
    "AP_10=[]\n",
    "AP_15=[]\n",
    "AP_20=[]\n",
    "AP_25=[]\n",
    "AP_30=[]\n",
    "AP_10_1=[]\n",
    "AP_15_1=[]\n",
    "AP_20_1=[]\n",
    "AP_25_1=[]\n",
    "AP_30_1=[]\n",
    "RA_10=[]\n",
    "RA_15=[]\n",
    "RA_20=[]\n",
    "RA_25=[]\n",
    "RA_30=[]\n",
    "RA_10_1=[]\n",
    "RA_15_1=[]\n",
    "RA_20_1=[]\n",
    "RA_25_1=[]\n",
    "RA_30_1=[]\n",
    "time_taken=[]\n",
    "# for i in range(845):\n",
    "for i in range(2000):\n",
    "    node_val.append(node_5[i])\n",
    "    P_10.append((prec1[i][0][0]+prec1[i][1][0]+prec1[i][2][0]))\n",
    "    P_15.append((prec1[i][0][1]+prec1[i][1][1]+prec1[i][2][1]))\n",
    "    P_20.append((prec1[i][0][2]+prec1[i][1][2]+prec1[i][2][2]))\n",
    "    P_25.append((prec1[i][0][3]+prec1[i][1][3]+prec1[i][2][3]))\n",
    "    P_30.append((prec1[i][0][4]+prec1[i][1][4]+prec1[i][2][4]))\n",
    "    R_10.append((recall1[i][0][0]+recall1[i][1][0]+recall1[i][2][0])/3)\n",
    "    R_15.append((recall1[i][0][1]+recall1[i][1][1]+recall1[i][2][1])/3)\n",
    "    R_20.append((recall1[i][0][2]+recall1[i][1][2]+recall1[i][2][2])/3)\n",
    "    R_25.append((recall1[i][0][3]+recall1[i][1][3]+recall1[i][2][3])/3)\n",
    "    R_30.append((recall1[i][0][4]+recall1[i][1][4]+recall1[i][2][4])/3)\n",
    "    AP_10.append((ave_prec3[i][0]+ave_prec3[i][5]+ave_prec3[i][10])/3)\n",
    "    AP_15.append((ave_prec3[i][1]+ave_prec3[i][6]+ave_prec3[i][11])/3)\n",
    "    AP_20.append((ave_prec3[i][2]+ave_prec3[i][7]+ave_prec3[i][12])/3)\n",
    "    AP_25.append((ave_prec3[i][3]+ave_prec3[i][8]+ave_prec3[i][13])/3)\n",
    "    AP_30.append((ave_prec3[i][4]+ave_prec3[i][9]+ave_prec3[i][14])/3)\n",
    "    AP_10_1.append((ave_prec4[i][0]+ave_prec4[i][5]+ave_prec4[i][10])/3)\n",
    "    AP_15_1.append((ave_prec4[i][1]+ave_prec4[i][6]+ave_prec4[i][11])/3)\n",
    "    AP_20_1.append((ave_prec4[i][2]+ave_prec4[i][7]+ave_prec4[i][12])/3)\n",
    "    AP_25_1.append((ave_prec4[i][3]+ave_prec4[i][8]+ave_prec4[i][13])/3)\n",
    "    AP_30_1.append((ave_prec4[i][4]+ave_prec4[i][9]+ave_prec4[i][14])/3)\n",
    "    RA_10.append((rank3[i][0]+rank3[i][5]+rank3[i][10])/3)\n",
    "    RA_15.append((rank3[i][1]+rank3[i][6]+rank3[i][11])/3)\n",
    "    RA_20.append((rank3[i][2]+rank3[i][7]+rank3[i][12])/3)\n",
    "    RA_25.append((rank3[i][3]+rank3[i][8]+rank3[i][13])/3)\n",
    "    RA_30.append((rank3[i][4]+rank3[i][9]+rank3[i][14])/3)\n",
    "    RA_10_1.append((rank4[i][0]+rank4[i][5]+rank4[i][10])/3)\n",
    "    RA_15_1.append((rank4[i][1]+rank4[i][6]+rank4[i][11])/3)\n",
    "    RA_20_1.append((rank4[i][2]+rank4[i][7]+rank4[i][12])/3)\n",
    "    RA_25_1.append((rank4[i][3]+rank4[i][8]+rank4[i][13])/3)\n",
    "    RA_30_1.append((rank4[i][4]+rank4[i][9]+rank4[i][14])/3)\n",
    "    time_taken.append(statistics.mean(time_exec1[i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f24268",
   "metadata": {},
   "outputs": [],
   "source": [
    "reccv_eval2 = pd.DataFrame({\n",
    "    'Paper_Id':node_val,\n",
    "    'Rec@10':R_10,\n",
    "    'Prec@10':P_10,\n",
    "    'AP@10':AP_10,\n",
    "    'RR@10':RA_10,\n",
    "    'AP1@10':AP_10_1,\n",
    "    'RR1@10':RA_10_1,\n",
    "    'Rec@15':R_15,\n",
    "    'Prec@15':P_15,\n",
    "    'AP@15':AP_15,\n",
    "    'RR@15':RA_15,\n",
    "    'AP1@15':AP_15_1,\n",
    "    'RR1@15':RA_15_1,\n",
    "    'Rec@20':R_20,\n",
    "    'Prec@20':P_20,\n",
    "    'AP@20':AP_20,\n",
    "    'RR@20':RA_20,\n",
    "    'AP1@20':AP_20_1,\n",
    "    'RR1@20':RA_20_1,\n",
    "    'Rec@25':R_25,\n",
    "    'Prec@25':P_25,\n",
    "    'AP@25':AP_25,\n",
    "    'RR@25':RA_25,\n",
    "    'AP1@25':AP_25_1,\n",
    "    'RR1@25':RA_25_1,\n",
    "    'Rec@30':R_30,\n",
    "    'Prec@30':P_30,\n",
    "    'AP@30':AP_30,\n",
    "    'RR@30':RA_30,\n",
    "    'AP1@30':AP_30_1,\n",
    "    'RR1@30':RA_30_1,\n",
    "    'Time':time_taken\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1500ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
